{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DuckDB: Moc hurtowni danych w Twoim laptopnie (PyData 2024) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show time execution under each cell.\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# in-memory connection\n",
    "duck = duckdb.connect()\n",
    "\n",
    "# persistant connection - db file will be created\n",
    "duck = duckdb.connect('demo.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading CSV file performance test: pandas, polars, spark, duckdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as rs\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_csv(\"data/transactions.csv\", header=None)\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polars_df = rs.read_csv(\"data/transactions.csv\", has_header=False)\n",
    "polars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(\"data/transactions.csv\", inferSchema=True, header=False)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck.sql(\"\"\"--sql\n",
    "\n",
    "    SELECT * FROM read_csv_auto('data/transactions.csv')         \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck.sql(\"\"\"--sql\n",
    "\n",
    "    SELECT * FROM read_csv('data/transactions.csv',\n",
    "        delim = ',',\n",
    "        header = false,\n",
    "        columns = {\n",
    "            \"product_id\": 'INT',\n",
    "            \"created_at\": \"DATE\",\n",
    "            \"has_discount\": 'BOOL',\n",
    "            \"location\": \"VARCHAR\",\n",
    "            \"brand\": \"VARCHAR\",\n",
    "            \"seller\": \"VARCHAR\",\n",
    "            \"amount\": \"DECIMAL\",\n",
    "            \"shipping_costs\": \"DECIMAL\",\n",
    "            \"wharehouse_location\": \"VARCHAR\",\n",
    "            \"is_shipped\": \"BOOL\"\n",
    "        }\n",
    "    )         \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create views and tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create View\n",
    "# This will still load data directly from file\n",
    "\n",
    "duck.sql(\"\"\"--sql\n",
    "\n",
    "    CREATE OR REPLACE VIEW transactions_view AS (\n",
    "        SELECT * FROM read_csv('data/transactions.csv',\n",
    "            delim = ',',\n",
    "            header = false,\n",
    "            columns = {\n",
    "                \"product_id\": 'INT',\n",
    "                \"created_at\": \"DATE\",\n",
    "                \"has_discount\": 'BOOL',\n",
    "                \"location\": \"VARCHAR\",\n",
    "                \"brand\": \"VARCHAR\",\n",
    "                \"seller\": \"VARCHAR\",\n",
    "                \"amount\": \"DECIMAL\",\n",
    "                \"shipping_costs\": \"DECIMAL\",\n",
    "                \"wharehouse_location\": \"VARCHAR\",\n",
    "                \"is_shipped\": \"BOOL\"\n",
    "            }\n",
    "        )\n",
    "    )       \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple query\n",
    "duck.sql(\"\"\"--sql\n",
    "\n",
    "    SELECT COUNT(*) FROM transactions_view\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex query\n",
    "# note: data was generated randomly\n",
    "duck.sql(\"\"\"--sql\n",
    "\n",
    "    SELECT\n",
    "        location, \n",
    "        SUM(\"amount\") as amount,\n",
    "        AVG(\"shipping_costs\") as avg_shipping_costs,\n",
    "    FROM transactions_view\n",
    "    WHERE \n",
    "        has_discount = true\n",
    "    GROUP BY\n",
    "        location\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data into DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table into DuckDB persistent storage\n",
    "# Convert CSV into DuckDB column oriented format\n",
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "   CREATE OR REPLACE TABLE transactions AS (\n",
    "       SELECT * FROM transactions_view\n",
    "   )\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same query after loading into DuckDB storage format (column-oriented)\n",
    "# Compression about 6x times. 1.2GB -> 200mb\n",
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "    SELECT\n",
    "        location, \n",
    "        SUM(\"amount\") as amount,\n",
    "        AVG(\"shipping_costs\") as avg_shipping_costs,\n",
    "    FROM transactions\n",
    "    WHERE \n",
    "        has_discount = true\n",
    "    GROUP BY\n",
    "        location\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over 200m rows\n",
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "    SELECT COUNT(*) FROM read_parquet(\"data/nyc_taxi/*.parquet\")\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "    SELECT\n",
    "        hvfhs_license_num,\n",
    "        pickup_datetime,\n",
    "        trip_miles,\n",
    "        trip_time\n",
    "    FROM read_parquet(\"data/nyc_taxi/*.parquet\")\n",
    "    LIMIT 5\n",
    "           \n",
    "\"\"\").show(max_width=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "        SELECT \n",
    "            date_trunc('month', pickup_datetime) as year_month,\n",
    "            ROUND(SUM(trip_miles)) as total_trip_miles, \n",
    "            ROUND(AVG(trip_miles), 2) avg_trip_miles,\n",
    "            ROUND(AVG(trip_time)/60, 2) avg_trip_time_minutes\n",
    "        \n",
    "        FROM read_parquet(\"data/nyc_taxi/*.parquet\")\n",
    "        GROUP BY\n",
    "            date_trunc('month', pickup_datetime)\n",
    "        ORDER BY \n",
    "            date_trunc('month', pickup_datetime)\n",
    "           \n",
    "\"\"\")\n",
    "\n",
    "# AWS Athena - 0.07 USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data to other formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view - just for conviniance\n",
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "    CREATE OR REPLACE VIEW nyc_taxi_report as (\n",
    "   \n",
    "        SELECT\n",
    "            CASE \n",
    "                WHEN hvfhs_license_num='HV0003' THEN 'Uber'\n",
    "                WHEN hvfhs_license_num='HV0005' THEN 'Lyft'\n",
    "            END as carrier,\n",
    "            ROUND(SUM(trip_miles)) as total_trip_miles, \n",
    "            ROUND(AVG(trip_miles), 2) avg_trip_miles,\n",
    "            ROUND(AVG(trip_time)/60, 2) avg_trip_time_minutes\n",
    "        \n",
    "        FROM read_parquet(\"data/nyc_taxi/*.parquet\")\n",
    "        GROUP BY\n",
    "            hvfhs_license_num\n",
    "    \n",
    "    )\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet, csv, json\n",
    "duck.sql(\"\"\"--sql\n",
    "   \n",
    "    COPY nyc_taxi_report TO 'nyc_taxi_report.parquet' (FORMAT PARQUET);\n",
    "    COPY nyc_taxi_report TO 'nyc_taxi_report.json';\n",
    "    COPY nyc_taxi_report TO 'nyc_taxi_report.csv' (HEADER, DELIMITER ',');\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to excel with spatial extention\n",
    "duck.sql(\"\"\"--sql\n",
    "         \n",
    "    -- install and load extentions\n",
    "    INSTALL spatial;\n",
    "    LOAD spatial;\n",
    "   \n",
    "    -- save as xlsx\n",
    "    COPY nyc_taxi_report TO 'nyc_taxi_report.xlsx' WITH (FORMAT GDAL, DRIVER 'xlsx');\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with sample data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'Age': [25, 30, 35, 22],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']\n",
    "}\n",
    "\n",
    "my_df = pd.DataFrame(data)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to pandas dataframes \n",
    "duck.sql(\"\"\"--sql\n",
    "         \n",
    "    SELECT * FROM my_df\n",
    "           \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = duck.sql(\"\"\"--sql\n",
    "  \n",
    "  -- How many trips per day for each carrier.\n",
    "  WITH trips__day_carrier as (\n",
    "        SELECT\n",
    "            date_trunc('day', Pickup_datetime) as pickup_date,\n",
    "            CASE \n",
    "                WHEN hvfhs_license_num='HV0003' THEN 'Uber'\n",
    "                WHEN hvfhs_license_num='HV0005' THEN 'Lyft'\n",
    "            END as carrier,\n",
    "            COUNT(*) as trips_count\n",
    "        \n",
    "        FROM read_parquet(\"data/nyc_taxi/*.parquet\")\n",
    "        GROUP BY\n",
    "            date_trunc('day', Pickup_datetime), hvfhs_license_num\n",
    "        ORDER BY\n",
    "            date_trunc('day', Pickup_datetime), hvfhs_license_num\n",
    "  ),\n",
    "  pivoted as (\n",
    "      PIVOT trips__day_carrier ON carrier USING SUM(trips_count)\n",
    "  )\n",
    "  SELECT * FROM pivoted\n",
    "  ORDER BY pickup_date\n",
    "           \n",
    "\"\"\").df()\n",
    "\n",
    "trips_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.line(trips_df, x='pickup_date', y=['Uber', 'Lyft'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark API\n",
    "\n",
    "drag & drop pySpark replacement (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckdb.experimental.spark.sql import SparkSession as session\n",
    "from duckdb.experimental.spark.sql.functions import lit, col\n",
    "\n",
    "# from pyspark.sql import SparkSession as session\n",
    "# from pyspark.sql.functions import lit, col\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "spark = session.builder.getOrCreate()\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'name': ['Joan', 'Peter', 'John', 'Bob'],\n",
    "    'age': [34, 45, 23, 56],\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df = df.withColumn(\n",
    "    'year', lit(2024) - col('age')\n",
    ")\n",
    "df = df.select(\n",
    "    col('name'),\n",
    "    col('age'),\n",
    "    col('year'),\n",
    "    lit('New York').alias('location')\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "duck.execute(\"INSTALL httpfs;\")\n",
    "duck.execute(\"LOAD httpfs;\")\n",
    "\n",
    "duck.execute(f\"SET s3_region='{os.environ['AWS_REGION']}';\")\n",
    "duck.execute(f\"SET s3_access_key_id='{os.environ['AWS_ACCESS_KEY_ID']}';\")\n",
    "duck.execute(f\"SET s3_secret_access_key='{os.environ['AWS_SECRET_ACCESS_KEY']}';\")\n",
    "\n",
    "\n",
    "duck.sql(\"\"\"--sql\n",
    "         \n",
    "    SELECT * FROM read_parquet(\"s3://skilzzz/sources/devitjobsuk/offers/parquet/devitjobsuk__offers.parquet\")\n",
    "\n",
    "\"\"\").show(max_width=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duckdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
